<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Quora Insincere Questions</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">


		<!-- Wrapper -->
		<div id="wrapper" class="fade-in">

			<!-- Intro -->
				<div id="intro">
					<h1>Quora Insincere Questions<br />
					</h1>
					<p>Portfolio of <a href="https://www.linkedin.com/in/aovaldes2/">Alejandro O. Valdés</a>, a mathematician specialized in data science.</p>
					<ul class="actions">
						<li><a href="DS_CV.pdf" class="button icon solid fa-download">Curriculum Vitae</a></li>
					</ul>
					<ul class="actions">
						<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
					</ul>
				</div>

			<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo">Return to Main Page</a>
				</header>

			<!-- Nav -->
			<nav id="nav">
				<ul class="links">
					<li class="active"><a href="QInsincereQuestion.html">Quora Insincere Questions</a></li>
					<li><a href="GenomesandGenetics.html">Of Genomes and Genetics</a></li>
					<li><a href="FCTPrediction.html">Forest Cover Type</a></li>
					<li><a href="NYCTaxiTripDuration.html">NYC Taxi Trip Duration</a></li>
				</ul>
				<ul class="icons">
					<li><a href="https://www.linkedin.com/in/aovaldes2/" class="icon brands alt fa-linkedin"><span class="label">Instagram</span></a></li>
					<li><a href="https://github.com/aovaldes2" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
				</ul>
			</nav>

			<!-- Main -->
				<div id="main">


					<!-- Post -->
					<section class="post">
						<header class="major">
							<h2><a href="https://github.com/aovaldes2/Data-Science-Portfolio/tree/main/Of%20Genomes%20And%20Genetics"><br/>Quora Insincere Questions</a></h2>
						<a href="images\imgQuora0.png" class="image fit"><img src="images\imgQuora0.png" alt="" /></a>
						</header>


						<p>Kaggle Competition hosted by Quora for improving its online conversations  <a href="https://www.kaggle.com/competitions/quora-insincere-questions-classification/overview"><b>[link]</b></a>.</p>
						
						<p>Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. 
							A key challenge is to weed out insincere questions — those founded upon false premises, or that intend to make a statement rather than looking for helpful answers.</p>
						
						<p>As this was a kernel only competition, external data sources were not allowed. We have to submit the kaggle kernel(either notebook or script) with all the code and output predictions in the specific format as mentioned in the submission requirements.</p>
						
						<p>In this competition it will be develop models that identify and flag insincere questions.</p>
					
						
						
							<h3>General Description</h3>

							
						
							<p>An insincere question is defned as a question intended to make a statement rather than look for helpful answers. Some characteristics tha can signify that a question is insincere:</p>
						
							<h5>Has a non-neutral tone</h5>
							<div class="row">
								<ul>
									<li>Has an exaggerated tone to underscore a point about a group of people.</li>
									<li>Is rhetorical and meant to impy a statement about a group of people.</li>
								</ul>
							</div>
						
							<h5>Is disparaging or inflammatory</h5>
							<div class="row">
								<ul>
									<li>Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype.</li>
									<li>Makes desparaging attacks/insults against a specific person or group of people.</li>
									<li>Disparages against a characteristic that is not fixable and not measurable.</li>
								</ul>
							</div>
							
							<h5>Isn't grounded in reality</h5>
							<div class="row">
								<ul>
									<li>Based on false information, or contains absurd assumptions.</li>
								</ul>
							</div>

							<h3>File Description</h3>

							<div class="row">
								<ul>
									<li><b>train.csv</b> - the training set.</li>
									<li><b>test.csv</b> - the test set.</li>
									<li><b>sample_submission.csv</b> - A sample submission in the correct format.</li>
								</ul>
							</div>

							
						<h3>Performance Metric</h3>
						
						<p> Source:<a href="https://www.kaggle.com/c/quora-insincere-questions-classification/overview/evaluation"><b>https://www.kaggle.com/c/quora-insincere-questions-classification/overview/evaluation</b></a></p>
						
						<p>- [F1 Score] <a href="https://en.wikipedia.org/wiki/F1_score"><b>(https://en.wikipedia.org/wiki/F1_score)</b></a></p>
						
						<p>Along with the question's text data, quora had also provided 4 different embedding files trained on large corpus of data that can be used in the models. Given embedding files are as follows:</p>
						
						<div class="row">
							<ul>
								<li >Google News — vectors</li>
								<li>Glove</li>
								<li>Paragram</li>
								<li>Wiki-news</li>
							</ul>
							</div>
						
						In each of these embedding files words are represented as 300 dim vectors. This representation of words will allow to capture semantic meaning of words. Words with same meaning will have a similar vector representation.
						
						<h3>Exploratory Data Analysis</h3>
						
						<b>Data fields</b>
						<div class="row">
							<ul>	
							<li><b>qid</b>- unique question identifier.</li>
							<li><b>question_text</b> - Quora question text.</li>
							<li><b>target</b> - A question labeled "insincere" has a value of 1, otherwise 0.</li>
							</ul>
							</div>

						
						<h3>Distribution of data points among output class</h3>
						
						<a href="./DS_images/img1Data%5BQIQ%5D.png" class="image main"><img src="./DS_images/img1Data%5BQIQ%5D.png" alt="img6" /></a>
						
						<h3>Word cloud for both sincere and insincere questions</h3>
						
						<a href="./DS_images/img23DataQ%5BQIQ%5D.png" class="image main"><img src="./DS_images/img23DataQ%5BQIQ%5D.png" alt="img7" /></a>
						
						<h3>Analysis on extracted features</h3>
						

						
						<h5>Basic Feauture Extraction(before cleaning):</h5>
						<div class="row">
							<ul style="list-style-type:square">
							<li><b>num_words</b>= Number of words in question.</li>
							<li><b>num_capital_let</b>= Number of capital letters in the question.</li>
							<li><b>num_special_char</b> = Number of special characters in the question.</li>
							<li><b>num_unique_words</b> = Number of unique words in the question.</li>
							<li><b>num_numerics</b> = Number of numerics in the question.</li>
							<li><b>num_char</b> = Number of characters in Question.</li>
							<li><b>num_stopwords</b> = Number of stopwords in the question.</li>
							</ul>
							</div>

							<a href="./DS_images/img4Datagrams%5BQIQ%5D.png" class="image main"><img src="./DS_images/img4Datagrams%5BQIQ%5D.png" alt="img8" /></a>
						
						
						
						<h5>Observations from the EDA:</h5>
						
						<div class="row">
								<ul>
									<li>Data is highly imbalanced with only 6.2% of insincere questions.</li>
									<li>F1-Score seems to be right choice than accuracy here because of data imbalance.</li>
									<li>As we can see insincere questions contain many of the offensive words.</li>
									<li>Most of the questions are related to *People*, *Muslim*,  *Women*, *India*, *Trump*, etc.</li>
									<li>Insincere questions seems to have more words and characters.</li>
									<li>Insincere questions also have more unique words compare to sincere questions.</li>
									<li>Looks like there are some math questions(most of them are classified as insincere) in the data which contains more special chars and numbers.</li>
									<li>Some questions also contains emojis and non-english characters.</li>
								</ul>
							</div>
						
						<h4>Data Preprocessing and Cleaning</h4>
						
						<p>The data is quite messy, there are lots of words that are mispelled, and some special symbols, which can not got corresponding embeddings, so before put into model, we need to clean those mispelled words and clean out the special symbols. Also, those mispelled words and special symbols could have some information, e.g., questions have mispelled words or special symbols would be more possible to be an insincere question, so I also marked them during cleaning up process.</p>
						
						<p>The following steps have been followed:</p>

						<div class="row">
							<ul style="list-style-type:disc">	
							<li>Replacing math equations with common abbrevation.</li>
							<li>Removing punctuations.</li>
							<li>Removing Numbers.</li>
							<li>Cleaning contractions.</li>
							<li>Feature extraction II.</li>
							<li>Spell Correction.</li>
							<li>Data Cleaning.</li>
							</ul>
							</div>					   
						
						<p>As a good practice in this case, Spacy was used for the creation of our words sequences,  word and lemma dictionaries. These 2 dictionaries were later used to create the embedding matrix. During our experimentation, a method was used in which many operations were performed, but in our most recent version at the end of the method itself they were ignored by example other features that could be useful in other resolutions.</p>
						
						<h3>Embedding</h3>
						
						<p>For increasing embeddings coverage I have used the combination of word stemming, lemmatization, capitalization, lowercase, uppercase as well as embedding of the nearest word using spell checker(using the wiki-news embedding) to get embeddings for all words in vocab. Created two separate embedding matrices with Glove and Paragran embedding files. Finally, taken weighted average of them giving higher weightage to glove.</p>

<pre><code>
embedding_matrix_glove, nb_words = load_embeddings(word_dict, lemma_dict, 'glove')
embedding_matrix_para, nb_words = load_embeddings(word_dict, lemma_dict, 'para')
embedding_matrix = np.mean((1.28*embedding_matrix_glove, 0.72*embedding_matrix_para), axis=0)
</code></pre>

						<h3>Models</h3>
						
						<p>After creating the embedding matrix, I built an ensemble of two different models' architectures to capture different aspects of the dataset and thus increasing overall F1-Score. In order to do that, I used Blending technique, which uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models.</p>
						
						<p>At first, I developped three models, one of the <b>TextCNN</b> type and the other two of the <b>Bidirectional RNN(LSTM)</b> type. Finally, I got the best results with an emsemble of the last two as I mentioned before.</p>
						
						<p>Let's take a closer look at these last two models.</p>
						
						<h4>Bidirectional RNN(LSTM)</h4>
						
						<p>Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous steps are fed as input to the current step, thus remembers some information about the sequence. It has limitations like difficulty in remembering longer sequences. LSTM/GRU are improved versions of RNN, specialized in remembering information for an extended period using a gating mechanism which RNN fails to do.</p>
						
						<p>Unidirectional RNN's only preserves information of the past because the inputs it has seen are from the past. Using bidirectional will run the inputs in two ways, one from past to future and one from future to past allowing it to preserve contextual information from both past and future at any point of time.</p>
						
						<h5>Arquitecture</h5>

<pre><code>
max_features_temp = len(word_dict)+1

inp = Input(shape=(params['maxlen'],))
x = Embedding(max_features_temp, params['embed_size'], weights=[embedding_matrix])(inp)
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = GlobalMaxPool1D()(x)
x = Dense(16, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(1, activation="sigmoid")(x)
model1 = Model(inputs=inp, outputs=x)
model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>
					   
						
							<h4>Bidirectional RNN(LSTM) II</h4>
							<p>This model consists of Bidirectional LSTM followed by Convolutional and pooling layers. The idea behind using CNNs in NLP is to make use of their ability to extract features. CNNs are applied to embedding vectors of a given sentence with the hopes that they'll manage to extract useful features(such as phrases and relationships between words that are closer together in the sentence) which can be used for text classification.</p>
						
							<p>The NLP CNN is usually made up of 3 or more 1D convolutional and pooling layers unlike traditional CNNs. This helps reduce the dimensionality of the text and acts as a summary of sorts which is then fed to a series of dense layers.</p>
						
						<h5>Arquitecture</h5> 

						<pre><code>max_features_temp = len(word_dict)+1


spatialdropout=0.2 
rnn_units=128 
filters=[100, 80, 30, 12] 

inp = Input(shape=(params['maxlen'],))    
	
emb = Embedding(max_features_temp, params['embed_size'], weights=[embedding_matrix], name='Embedding')(inp)

x = SpatialDropout1D(rate=spatialdropout, seed=10000)(emb)

rnn = Bidirectional(CuDNNLSTM(rnn_units, return_sequences=True, kernel_initializer=initializers.glorot_uniform(seed=123000), recurrent_initializer=initializers.Orthogonal(gain=1.0, seed=123000)))(x)

x1 = Conv1D(filters=filters[0], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=110000))(rnn)

x2 = Conv1D(filters=filters[1], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=120000))(rnn)

x3 = Conv1D(filters=filters[2], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=130000))(rnn)

x4 = Conv1D(filters=filters[3], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=140000))(rnn)

x1 = GlobalMaxPooling1D()(x1)
x2 = GlobalMaxPooling1D()(x2)
x3 = GlobalMaxPooling1D()(x3)
x4 = GlobalMaxPooling1D()(x4)

c = concatenate([x1, x2, x3, x4])
x = Dense(200, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=111000))(c)
x = Dropout(0.2, seed=10000)(x)
x = BatchNormalization()(x)
x_output = Dense(1, activation='sigmoid', kernel_initializer=initializers.glorot_uniform(seed=110000))(x)

model2 = Model(inputs=inp, outputs=x_output)

model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
							</code></pre>

						
						<h3>Blending</h3>
						
						<p>In this case its used a simple lineal combination of the two best models looking for a better score.</p>
<pre><code>
final_preds_val = 0.5*pred_modelBRNN_x_val+0.5*pred_modelBRNN2_x_val
final_preds = 0.5*pred_modelBRNN+0.5*pred_modelBRNN2
</code></pre>
							<h3>Submission</h3>
						
							<p>The metric used in the competition is the F-1 Score, which could not be directly optimized. To approximately optimize it, I tried to find an optimal threshold to make the hard classification. But one tricky thing is, a small validation set is needed to find the “best threshold”, and this threshold could vary significantly for different training.</p>
						
						<h3>Results</h3>
							<p>The TextCNN was descarted because it have too poorly results compared with the other models, the named Bidirectional RNN(LSTM) have an score of 0.66684 with an threshold at 0.5 meanwhile the Bidirectional RNN(LSTM)II have 0.67245 with an threshold at 0.35 then the Blending model resultant of a simple lineal combination of the two latets models had the best scores with 0.68091(threshold of 0.35) and 0.68231(threshold of 0.32).</p>
						
							<a href="./DS_images/imgQuoraSubmissions%5BQIQ%5D.png" class="image main"><img src="./DS_images/imgQuoraSubmissions%5BQIQ%5D.png" alt="img9" /></a>
						
						<h3>Conclusion:</h3>
						<p>This competition had as its main peculiarity that the solutions should run with the competitor's kernels only. My solution focused on deep learning methods, and explore widely used methodologies in this field such as bidirectional RNN, Convolutions layers, Pooling layers etc. It has been an opportunity to learn a lot from kaggle forums and public solutions. Finding more word embeddings and model assembly were the key factors for improving the F1-score.
						
						The F1-score obtained from the blending method performed better than any other model that i tried, giving a private score of 0.68231. Although it could be considered a good score for the problem in question its a little far from the winnings scores(0.71323).
						
						Additional improvements can be achieved by continuing to experiment with more combinations of the layers that were used in this work, as well as using different combinations of word embeddings.</p>

						
						
						<ul class="actions special">
							<li><a href="https://github.com/aovaldes2/Data-Science-Portfolio/blob/main/Quora_Insincere_Questions.ipynb" class="button large">View Project Code</a></li>
						</ul>
					



					</section>



						



					


					


				</div>

			<!-- Footer -->
				<footer id="footer">
					<section>
						<form method="post" action="https://formsubmit.co/aovaldes2@gmail.com">
							<input type="hidden" name="_subject" value="New contact from the Data-Science-Portfolio website!!">
							<div class="fields">
								<div class="field">
									<label for="name">Name</label>
									<input type="text" name="name" id="name" placeholder="Name" required/>
								</div>
								<div class="field">
									<label for="email">Email</label>
									<input type="text" name="email" id="email" placeholder="Email Address" required/>
								</div>
								<div class="field">
									<label for="message">Message</label>
									<textarea name="message" id="message" rows="3" placeholder="Write your message to Alejandro here" required></textarea>
								</div>
							</div>
							<ul class="actions">
								<li><input type="submit" value="Send Message" /></li>
							</ul>
						</form>
					</section>
					<section class="split contact">
						<section class="alt">
							<h3>Address</h3>
							<p>Leganés, CP 28911<br />
							Madrid, España</p>
						</section>
						<section>
							<h3>Phone</h3>
							<p><a href="#">(+34) 631-327628</a></p>
						</section>
						<section>
							<h3>Email</h3>
							<p><a href="#">aovaldes2@gmail.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
								<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
								<li><a href="https://www.linkedin.com/in/aovaldes2/" class="icon brands alt fa-linkedin"><span class="label">Instagram</span></a></li>
								<li><a href="https://github.com/aovaldes2" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>

			<!-- Copyright -->
				<div id="copyright">
					<ul><li>&copy; Home Cuba</li><li>Maintained by: <a href="https://www.linkedin.com/in/aovaldes2/">Alejandro O. Valdés</a></li></ul>
				</div>

		</div>

	<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

</body>
</html>
